entry_id,published_year,authors,pdf_url,title,summary
https://arxiv.org/abs/2108.11510v1,2023,"John Doe, Jane Smith",https://arxiv.org/pdf/2108.11510v1,"Deep Reinforcement Learning in Computer Vision:
Deep Reinforcement Learning in Computer Vision:A Comprehensive Survey","This paper provides an overview of recent progress in deep reinforcement learning, including novel algorithms and applications."
https://arxiv.org/abs/2403.15790v1,2022,"Alice Johnson, Bob Lee",https://arxiv.org/pdf/2403.15790v1,"Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled
  Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets",An in-depth analysis of self-supervised learning approaches in computer vision and their effectiveness in downstream tasks.
https://arxiv.org/abs/2103.08765,2021,"Michael Brown, Emily White",https://arxiv.org/pdf/2103.08765,Transformer Models for Time Series Forecasting,"This paper examines the use of transformers for time series forecasting, comparing them with traditional statistical models."
http://arxiv.org/abs/2008.06693v4,2020,"['Alexandre Heuillet', 'Fabien Couthouis', 'Natalia DÃ­az-RodrÃ­guez']",http://arxiv.org/pdf/2008.06693v4,Explainability in Deep Reinforcement Learning,"A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainaility. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems."
http://arxiv.org/abs/2008.07463v1,2020,"['Sabiha Tahrat', 'German Braun', 'Alessandro Artale', 'Marco Gario', 'Ana Ozaki']",http://arxiv.org/pdf/2008.07463v1,Automated Reasoning in Temporal DL-Lite,"This paper investigates the feasibility of automated reasoning over temporal DL-Lite (TDL-Lite) knowledge bases (KBs). We test the usage of off-the-shelf LTL reasoners to check satisfiability of TDL-Lite KBs. In particular, we test the robustness and the scalability of reasoners when dealing with TDL-Lite TBoxes paired with a temporal ABox. We conduct various experiments to analyse the performance of different reasoners by randomly generating TDL-Lite KBs and then measuring the running time and the size of the translations. Furthermore, in an effort to make the usage of TDL-Lite KBs a reality, we present a fully fledged tool with a graphical interface to design them. Our interface is based on conceptual modelling principles and it is integrated with our translation tool and a temporal reasoner."
http://arxiv.org/abs/2301.06845v1,2023,"['Sander Beckers', 'Joseph Y. Halpern', 'Christopher Hitchcock']",http://arxiv.org/pdf/2301.06845v1,Causal Models with Constraints,"Causal models have proven extremely useful in offering formal representations of causal relationships between a set of variables. Yet in many situations, there are non-causal relationships among variables. For example, we may want variables $LDL$, $HDL$, and $TOT$ that represent the level of low-density lipoprotein cholesterol, the level of lipoprotein high-density lipoprotein cholesterol, and total cholesterol level, with the relation $LDL+HDL=TOT$. This cannot be done in standard causal models, because we can intervene simultaneously on all three variables. The goal of this paper is to extend standard causal models to allow for constraints on settings of variables. Although the extension is relatively straightforward, to make it useful we have to define a new intervention operation that $disconnects$ a variable from a causal equation. We give examples showing the usefulness of this extension, and provide a sound and complete axiomatization for causal models with constraints."
http://arxiv.org/abs/2301.07345v1,2023,"['Irfansha Shaik', 'Valentin Mayer-Eichberger', 'Jaco van de Pol', 'Abdallah Saffidine']",http://arxiv.org/pdf/2301.07345v1,Implicit State and Goals in QBF Encodings for Positional Games (extended version),"We address two bottlenecks for concise QBF encodings of maker-breaker positional games, like Hex and Tic-Tac-Toe. Our baseline is a QBF encoding with explicit variables for board positions and an explicit representation of winning configurations. The first improvement is inspired by lifted planning and avoids variables for explicit board positions, introducing a universal quantifier representing a symbolic board state. The second improvement represents the winning configurations implicitly, exploiting their structure. The paper evaluates the size of several encodings, depending on board size and game depth. It also reports the performance of QBF solvers on these encodings. We evaluate the techniques on Hex instances and also apply them to Harary's Tic-Tac-Toe. In particular, we study scalability to 19$\times$19 boards, played in human Hex tournaments."
http://arxiv.org/abs/2301.07427v1,2023,"['Martina Cinquini', 'Fosca Giannotti', 'Riccardo Guidotti']",http://arxiv.org/pdf/2301.07427v1,Boosting Synthetic Data Generation with Effective Nonlinear Causal Discovery,"Synthetic data generation has been widely adopted in software testing, data privacy, imbalanced learning, and artificial intelligence explanation. In all such contexts, it is crucial to generate plausible data samples. A common assumption of approaches widely used for data generation is the independence of the features. However, typically, the variables of a dataset depend on one another, and these dependencies are not considered in data generation leading to the creation of implausible records. The main problem is that dependencies among variables are typically unknown. In this paper, we design a synthetic dataset generator for tabular data that can discover nonlinear causalities among the variables and use them at generation time. State-of-the-art methods for nonlinear causal discovery are typically inefficient. We boost them by restricting the causal discovery among the features appearing in the frequent patterns efficiently retrieved by a pattern mining algorithm. We design a framework for generating synthetic datasets with known causalities to validate our proposal. Broad experimentation on many synthetic and real datasets with known causalities shows the effectiveness of the proposed method."
http://arxiv.org/abs/2301.07629v1,2023,"['David M. Cerna', 'Andrew Cropper']",http://arxiv.org/pdf/2301.07629v1,Generalisation Through Negation and Predicate Invention,"The ability to generalise from a small number of examples is a fundamental challenge in machine learning. To tackle this challenge, we introduce an inductive logic programming (ILP) approach that combines negation and predicate invention. Combining these two features allows an ILP system to generalise better by learning rules with universally quantified body-only variables. We implement our idea in N OPI which can learn normal logic programs with negation and predicate invention, including Datalog with stratified negation. Our experimental results on multiple domains show that our approach improves predictive accuracies and learning times."
